{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_input = []\n",
    "l = []\n",
    "yelp_path = './yelp_labelled.txt'\n",
    "with open(yelp_path) as yp:\n",
    "    line = yp.readline()\n",
    "    while line:\n",
    "        yelp_input.append(line.strip())\n",
    "        line = yp.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataSet(ds):\n",
    "    data = [j[:-3] for j in ds]\n",
    "    data = [item.lower() for item in data]\n",
    "    for i in range(len(data)):\n",
    "        data[i] = re.sub('[^A-Za-z0-9-\\s]+', '', data[i])\n",
    "    k = [j[-1:] for j in ds]\n",
    "    for i in range(0, len(k)): \n",
    "        k[i] = int(k[i])\n",
    "    ds = list(zip(data , k))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSet(yelp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(ds):\n",
    "    ds_1 = []\n",
    "    ds_0 = []\n",
    "    \n",
    "    for i in range(len(ds)):\n",
    "        if ds[i][1] == 1:\n",
    "            ds_1.append(ds[i])\n",
    "        else:\n",
    "            ds_0.append(ds[i])\n",
    "    \n",
    "    ds_1 = random.sample(ds_1, len(ds_1))\n",
    "    \n",
    "    ds_0 = random.sample(ds_0, len(ds_0))\n",
    "    \n",
    "    return ds_1, ds_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1, ds_0 = dist(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ds, val):\n",
    "    index = int(val*len(ds))\n",
    "    train = ds[:index]\n",
    "    test = ds[index:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(ds_1, 0.7)\n",
    "train0, test0 = train_test_split(ds_0, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_data(input_data):\n",
    "    words = {}\n",
    "    tot_words = 0\n",
    "    for i in range(len(input_data)):\n",
    "        for w in input_data[i][0].split(\" \"):\n",
    "            tot_words += 1\n",
    "            if(w in words):\n",
    "                words[w] += 1\n",
    "            else:\n",
    "                words[w] = 1\n",
    "    \n",
    "    return words, tot_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1, tot_word1 = token_data(train1)\n",
    "word_0, tot_word0 = token_data(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1 + train0\n",
    "train_random = random.sample(train, len(train))\n",
    "\n",
    "test = test1 + test0\n",
    "test_random = random.sample(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = []\n",
    "TrainLabels = []\n",
    "TestData = []\n",
    "TestLabels = []\n",
    "\n",
    "\n",
    "for i in range(len(train_random)):\n",
    "    TrainData.append(train_random[i][0])\n",
    "    TrainLabels.append(train_random[i][1])\n",
    "    \n",
    "\n",
    "for i in range(len(test_random)):\n",
    "    TestData.append(test_random[i][0])\n",
    "    TestLabels.append(test_random[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(input_data):\n",
    "    voc = {}\n",
    "    tot_words = 0\n",
    "    for i in range(len(input_data)):\n",
    "        for w in input_data[i][0].split(\" \"):\n",
    "            tot_words += 1\n",
    "            if(w in voc):\n",
    "                voc[w] += 1\n",
    "            else:\n",
    "                voc[w] = 1\n",
    "    \n",
    "    return voc, tot_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tot_words = dictionary(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_calc(trainClass, data):\n",
    "    L = math.log(len(trainClass)/len(data))\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L0 = prior_calc(train0, train)\n",
    "L1 = prior_calc(train1, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mL0, mL1, token0, token1, test):\n",
    "    y = []\n",
    "    acc = 0\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        pred0 = mL0\n",
    "        pred1 = mL1\n",
    "    \n",
    "        for w in test[i][0].split(\" \"):\n",
    "            w = w.lower()\n",
    "            if token0.__contains__(w):\n",
    "                pred0 += token0[w]\n",
    "            if token1.__contains__(w):    \n",
    "                pred1 += token1[w]\n",
    "    \n",
    "        if pred0 > pred1:\n",
    "            y.append(0)\n",
    "        else: \n",
    "            y.append(1)\n",
    "        \n",
    "        if y[i] == test[i][1]:\n",
    "            acc += 1\n",
    "    \n",
    "    total_acc = acc/len(test)\n",
    "    \n",
    "    return total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mAP calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map(voc_data, voc_class, tot_words, k):\n",
    "    Acc_tokens = {}\n",
    "    voc_len = len(voc_data)\n",
    "    \n",
    "    for w in voc_data:\n",
    "        if voc_class.__contains__(w):\n",
    "            Acc_tokens[w] = math.log((voc_class[w] + k)/(tot_words + (voc_len*k)))\n",
    "        else:\n",
    "            if k != 0:\n",
    "                Acc_tokens[w] = math.log((k)/(tot_words + (voc_len*k)))\n",
    "    return Acc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_token0 = Map(voc, word_0, tot_word0, 0)\n",
    "L_token1 = Map(voc, word_1, tot_word1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map_token0 = Map(voc, word_0, tot_word0, 1)\n",
    "Map_token0 = Map(voc, word_1, tot_word1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation(t0,t1,a,m,k):\n",
    "    \n",
    "    new_train_0 = random.sample(t0,len(t0))\n",
    "    new_train_1 = random.sample(t1,len(t1))\n",
    "    \n",
    "    fold = int(len(new_train_0)/a)\n",
    "    \n",
    "    MAP_accuracy = np.zeros((10, 10))\n",
    "    \n",
    "    MAP_sm_accuracy = []\n",
    "    \n",
    "    sd_L = []\n",
    "    sd_map = []\n",
    "\n",
    "    cv_test0 = []\n",
    "    cv_test1 = []\n",
    "    cv_train0 = []\n",
    "    cv_train1 = []\n",
    "\n",
    "    for i in range(a):\n",
    "        if i == 0:\n",
    "            cv_test0 = new_train_0[(i*fold):((i+1)*fold)]\n",
    "            cv_train0 = new_train_0[(i+1)*fold:]\n",
    "\n",
    "            cv_test1 = new_train_1[i*fold:(i+1)*fold]\n",
    "            cv_train1 = new_train_1[(i+1)*fold:]\n",
    "        if i == (a-1):\n",
    "            cv_test0 = new_train_0[i*fold:(i+1)*fold]\n",
    "            cv_train0 = new_train_0[:(i)*fold]\n",
    "\n",
    "            cv_test1 = new_train_1[i*fold:(i+1)*fold]\n",
    "            cv_train1 = new_train_1[:(i)*fold]\n",
    "        else:\n",
    "            cv_test0 = new_train_0[i*fold:(i+1)*fold]\n",
    "            cv_train0 = new_train_0[:(i)*fold] + new_train_0[(i+1)*fold:]\n",
    "\n",
    "            cv_test1 = new_train_1[i*fold:(i+1)*fold]\n",
    "            cv_train1 = new_train_1[:(i)*fold] + new_train_1[(i+1)*fold:]\n",
    "            \n",
    "            \n",
    "        # Now we have cv_test_0, cv_train_0, cv_test_1, cv_train_1\n",
    "        if q == 1:\n",
    "            for l in range(10):\n",
    "\n",
    "                new_train_1 = cv_train_1[:int(((l+1)*len(cv_train_1)/10))]\n",
    "                new_train_0 = cv_train_0[:int(((l+1)*len(cv_train_0)/10))]\n",
    "\n",
    "                vocab,total_words = create_dict(new_train_1 + new_train_0)\n",
    "\n",
    "                vocab_1,total_words_1 = create_dict(new_train_1)\n",
    "                vocab_0,total_words_0 = create_dict(new_train_0)\n",
    "\n",
    "                maxL_0 = generate_prior(new_train_0,new_train_1+new_train_0)\n",
    "                maxL_1 = generate_prior(new_train_1,new_train_1+new_train_0)\n",
    "\n",
    "                MAP_tokens_0 = calculate_MAP(vocab,vocab_0,total_words_0,m)\n",
    "                MAP_tokens_1 = calculate_MAP(vocab,vocab_1,total_words_1,m)\n",
    "\n",
    "                MAP_curve_accuracy[i][l] = prediction(maxL_0,maxL_1,MAP_tokens_0,MAP_tokens_1,cv_test_1+cv_test_0)\n",
    "                \n",
    "            \n",
    "        if q == 2:\n",
    "            new_train_1 = cv_train_1\n",
    "            new_train_0 = cv_train_0\n",
    "\n",
    "            vocab,total_words = create_dict(new_train_1 + new_train_0)\n",
    "\n",
    "            vocab_1,total_words_1 = create_dict(new_train_1)\n",
    "            vocab_0,total_words_0 = create_dict(new_train_0)\n",
    "\n",
    "            maxL_0 = generate_prior(new_train_0,new_train_1+new_train_0)\n",
    "            maxL_1 = generate_prior(new_train_1,new_train_1+new_train_0)\n",
    "\n",
    "            MAP_tokens_0 = calculate_MAP(vocab,vocab_0,total_words_0,m)\n",
    "            MAP_tokens_1 = calculate_MAP(vocab,vocab_1,total_words_1,m)\n",
    "\n",
    "            MAP_smooth_accuracy.append(prediction(maxL_0,maxL_1,MAP_tokens_0,MAP_tokens_1,cv_test_1+cv_test_0))\n",
    "    \n",
    "    if q == 2:\n",
    "        mean = np.mean(MAP_smooth_accuracy,axis=0)\n",
    "        sd = np.std(MAP_smooth_accuracy,axis=0)\n",
    "        return mean,sd\n",
    "    \n",
    "    if q ==1:\n",
    "        mean = np.mean(MAP_curve_accuracy, axis=0)\n",
    "        sd = np.std(MAP_curve_accuracy, axis=0)\n",
    "        return mean,sd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
